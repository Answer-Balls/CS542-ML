{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 1: Linear Regression\n",
    "\n",
    "To run and solve this assignment, one must have a working IPython Notebook installation. The easiest way to set it up for both Windows and Linux is to install [Anaconda](https://www.continuum.io/downloads). Then save this file to your computer, run Anaconda and choose this file in Anaconda's file explorer. Use `Python 3` version. Below statements assume that you have already followed these instructions. If you are new to Python or its scientific library, Numpy, there are some nice tutorials [here](https://www.learnpython.org/) and [here](http://www.scipy-lectures.org/).\n",
    "\n",
    "To run code in a cell or to render [Markdown](https://en.wikipedia.org/wiki/Markdown)+[LaTeX](https://en.wikipedia.org/wiki/LaTeX) press `Ctr+Enter` or `[>|]`(like \"play\") button above. To edit any code or text cell double click on its content. To change cell type, choose \"Markdown\" or \"Code\" in the drop-down menu above. Here are some useful resources for [Markdown guide](https://www.markdownguide.org/basic-syntax/) and [LaTeX tutorial](https://www.overleaf.com/learn/latex/Learn_LaTeX_in_30_minutes) if you are not familiar with the basic syntax.\n",
    "\n",
    "If certain output is given for some cells, that means that you are expected to get similar results.\n",
    "\n",
    "Only **PDF** files are accepted for ps1 submission. To print this notebook to a pdf file, you can go to \"File\" -> \"Download as\" -> \"PDF via LaTex(.pdf)\" or simply use \"print\" in browser. \n",
    "\n",
    "Total: 185 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Numpy Tutorial\n",
    "**1\\.1 [5pt]**\n",
    "Modify the cell below to return a 5x5 matrix of ones. Put some code there and press `Ctrl+Enter` to execute contents of the cell. You should see something like the output above. [[1]](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.creation.html#arrays-creation) [[2]](https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.array-creation.html#routines-array-creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample = np.ones((5,5),dtype = float)\n",
    "\n",
    "print(sample)\n",
    "\n",
    "#raise NotImplementedError(\"Replace this raise statement with the code \"\n",
    "                          #\"that prints 5x5 matrix of ones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 [5pt]** \n",
    "Vectorizing your code is very important to get results in a reasonable time. Let A be a 10x10 matrix and x be a 10-element column vector. Your friend writes the following code. How would you vectorize this code to run without any for loops? Compare execution speed for different values of `n` with [`%timeit`](http://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.27319241]\n",
      " [2.72447252]\n",
      " [2.31508864]\n",
      " [1.6666262 ]\n",
      " [1.76466293]\n",
      " [2.49858266]\n",
      " [2.23126596]\n",
      " [1.79688772]\n",
      " [2.43382372]\n",
      " [1.14365312]]\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "def compute_something(A, x):\n",
    "    v = np.zeros((n, 1))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            v[i] += A[i, j] * x[j]\n",
    "    return v\n",
    "            \n",
    "A = np.random.rand(n, n)\n",
    "x = np.random.rand(n, 1)\n",
    "print(compute_something(A, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.27319241]\n",
      " [2.72447252]\n",
      " [2.31508864]\n",
      " [1.6666262 ]\n",
      " [1.76466293]\n",
      " [2.49858266]\n",
      " [2.23126596]\n",
      " [1.79688772]\n",
      " [2.43382372]\n",
      " [1.14365312]]\n"
     ]
    }
   ],
   "source": [
    "def vectorized(A, x):\n",
    "    return (A.dot(x))\n",
    "\n",
    "print(vectorized(A, x))\n",
    "assert np.max(abs(vectorized(A, x) - compute_something(A, x))) < 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.8 µs ± 3.39 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n",
      "The slowest run took 41.07 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "13.3 µs ± 23.2 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n",
      "---\n",
      "311 µs ± 21.3 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n",
      "The slowest run took 12.35 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "32.2 µs ± 22.8 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n",
      "---\n",
      "30.9 ms ± 635 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n",
      "The slowest run took 56.61 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "25.5 µs ± 45 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n",
      "---\n",
      "789 ms ± 19.7 ms per loop (mean ± std. dev. of 7 runs, 5 loops each)\n",
      "The slowest run took 8.46 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "156 µs ± 161 µs per loop (mean ± std. dev. of 7 runs, 5 loops each)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for n in [5, 10, 100, 500]:\n",
    "    A = np.random.rand(n, n)\n",
    "    x = np.random.rand(n, 1)\n",
    "    %timeit -n 5 compute_something(A, x)\n",
    "    %timeit -n 5 vectorized(A, x)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Linear regression with one variable\n",
    "\n",
    "In this part of this exercise, you will implement linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities. You would like to use this data to help you select which city to expand to next. The file ex1data.txt contains the dataset for our linear regression problem. The first column is the population of a city and the second column is the profit of a food truck in that city. A negative value for profit indicates a loss.\n",
    "\n",
    "**2.1 [10pt]** Get a plot similar to below\n",
    ": [[1]](https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.scatter.html) [[2]](https://matplotlib.org/api/pyplot_api.html?highlight=xlim#matplotlib.pyplot.xlim) [[3]](https://matplotlib.org/api/pyplot_api.html?highlight=matplotlib%20pyplot%20xlabel#matplotlib.pyplot.xlabel)\n",
    "\n",
    "Before starting on any task, it is often useful to understand the data by visualizing it. For this dataset, you can use a scatter plot to visualize the data, since it has only two properties to plot (profit and population). Many other problems that you will encounter in real life are multi-dimensional and can’t be plotted on a 2-d plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97, 1) (97, 1) 97\n",
      "[[6.1101]\n",
      " [5.5277]\n",
      " [8.5186]\n",
      " [7.0032]\n",
      " [5.8598]\n",
      " [8.3829]\n",
      " [7.4764]\n",
      " [8.5781]\n",
      " [6.4862]\n",
      " [5.0546]] \n",
      " [[17.592 ]\n",
      " [ 9.1302]\n",
      " [13.662 ]\n",
      " [11.854 ]\n",
      " [ 6.8233]\n",
      " [11.886 ]\n",
      " [ 4.3483]\n",
      " [12.    ]\n",
      " [ 6.5987]\n",
      " [ 3.8166]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.loadtxt('ex1data1.txt', delimiter=',')\n",
    "X, y = data[:, 0, np.newaxis], data[:, 1, np.newaxis]\n",
    "n = data.shape[0]\n",
    "print(X.shape, y.shape, n)\n",
    "print(X[:10], '\\n', y[:10])\n",
    "\n",
    "plt.scatter(X,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** Gradient Descent\n",
    "\n",
    "In this part, you will fit the linear regression parameter $\\theta$ to our dataset using gradient descent.\n",
    "\n",
    "The objective of linear regression is to minimize the cost function\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i = 1}^{m} \\big(h(x^{(i)}; \\theta) - y^{(i)}\\big)^2$$\n",
    "where the hypothesis $h(x;\\theta)$ is given by the linear model ($x'$ has an additional fake feature always equal to '`1`')\n",
    "$$ h(x;\\theta) = \\theta^T x' = \\theta_0 + \\theta_1 x$$\n",
    "\n",
    "Recall that the parameters of your model are the $\\theta_j$ values. These are the values you will adjust to minimize cost J(θ). One way to do this is to use the gradient descent\n",
    "algorithm. In batch gradient descent algorithm, each iteration performs the update.\n",
    "\n",
    "$$ \\theta_j^{(k+1)} = \\theta_j^{(k)} - \\eta \\frac{1}{m} \\sum_i \\big(h(x^{(i)}; \\theta) - y^{(i)}\\big) x^{(i)}_j $$\n",
    "With each step of gradient descent, your parameter $\\theta_j$ come closer to the optimal values\n",
    "that will achieve the lowest cost J(θ).\n",
    "\n",
    "**2.2.1** **[5pt]** Where does this update rule comes from?\n",
    "\n",
    "**Solution** \n",
    "\n",
    "In the gradient descent, the $\\theta$ is updated iteratively to minimize the cost function until converge. Every time we update the $\\theta$ according to the gradient: \n",
    "\n",
    "$$ \\theta_{j}^{(k+1)} = \\theta_j^{(k)} - \\eta \\frac{\\partial}{\\partial{\\theta_j}}J(\\theta) \\\\ \n",
    "= \\theta_j^{(k)} - \\eta \\frac{\\partial}{\\partial{\\theta_j}} \\frac{1}{2m} \\sum_{i = 1}^{m} \\big(h(x^{(i)}; \\theta) - y^{(i)}\\big)^2 \\\\ = \\theta_j^{(k)} - \\eta \\frac{1}{m} \\sum_i \\big(h(x^{(i)}; \\theta) - y^{(i)}\\big) \\frac{\\partial}{\\partial{\\theta_j}}h(x^{(i)}; \\theta) \\\\ = \\theta_j^{(k)} - \\eta \\frac{1}{m} \\sum_i \\big(h(x^{(i)}; \\theta) - y^{(i)}\\big) x^{(i)}_j\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "**2.2.2** **[30pt]** Cost Implementation\n",
    "\n",
    "As you perform gradient descent to learn to minimize the cost function, it is helpful to monitor the convergence by computing the cost. In this section, you will implement a function to calculate $J(\\theta)$ so you can check the convergence of your gradient descent implementation.\n",
    "\n",
    "In the following lines, we add another dimension to our data to accommodate the intercept term and compute the prediction and the loss. As you are doing this, remember that the variables X and y are not scalar values, but matrices whose rows represent the examples from the training set. In order to get $x'$ [add a column](https://docs.scipy.org/doc/numpy/reference/generated/numpy.insert.html) of ones to the data matrix `X`.\n",
    "\n",
    "You should expect to see a cost of approximately 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.1101]\n",
      " [ 5.5277]\n",
      " [ 8.5186]\n",
      " [ 7.0032]\n",
      " [ 5.8598]\n",
      " [ 8.3829]\n",
      " [ 7.4764]\n",
      " [ 8.5781]\n",
      " [ 6.4862]\n",
      " [ 5.0546]\n",
      " [ 5.7107]\n",
      " [14.164 ]\n",
      " [ 5.734 ]\n",
      " [ 8.4084]\n",
      " [ 5.6407]\n",
      " [ 5.3794]\n",
      " [ 6.3654]\n",
      " [ 5.1301]\n",
      " [ 6.4296]\n",
      " [ 7.0708]\n",
      " [ 6.1891]\n",
      " [20.27  ]\n",
      " [ 5.4901]\n",
      " [ 6.3261]\n",
      " [ 5.5649]\n",
      " [18.945 ]\n",
      " [12.828 ]\n",
      " [10.957 ]\n",
      " [13.176 ]\n",
      " [22.203 ]\n",
      " [ 5.2524]\n",
      " [ 6.5894]\n",
      " [ 9.2482]\n",
      " [ 5.8918]\n",
      " [ 8.2111]\n",
      " [ 7.9334]\n",
      " [ 8.0959]\n",
      " [ 5.6063]\n",
      " [12.836 ]\n",
      " [ 6.3534]\n",
      " [ 5.4069]\n",
      " [ 6.8825]\n",
      " [11.708 ]\n",
      " [ 5.7737]\n",
      " [ 7.8247]\n",
      " [ 7.0931]\n",
      " [ 5.0702]\n",
      " [ 5.8014]\n",
      " [11.7   ]\n",
      " [ 5.5416]\n",
      " [ 7.5402]\n",
      " [ 5.3077]\n",
      " [ 7.4239]\n",
      " [ 7.6031]\n",
      " [ 6.3328]\n",
      " [ 6.3589]\n",
      " [ 6.2742]\n",
      " [ 5.6397]\n",
      " [ 9.3102]\n",
      " [ 9.4536]\n",
      " [ 8.8254]\n",
      " [ 5.1793]\n",
      " [21.279 ]\n",
      " [14.908 ]\n",
      " [18.959 ]\n",
      " [ 7.2182]\n",
      " [ 8.2951]\n",
      " [10.236 ]\n",
      " [ 5.4994]\n",
      " [20.341 ]\n",
      " [10.136 ]\n",
      " [ 7.3345]\n",
      " [ 6.0062]\n",
      " [ 7.2259]\n",
      " [ 5.0269]\n",
      " [ 6.5479]\n",
      " [ 7.5386]\n",
      " [ 5.0365]\n",
      " [10.274 ]\n",
      " [ 5.1077]\n",
      " [ 5.7292]\n",
      " [ 5.1884]\n",
      " [ 6.3557]\n",
      " [ 9.7687]\n",
      " [ 6.5159]\n",
      " [ 8.5172]\n",
      " [ 9.1802]\n",
      " [ 6.002 ]\n",
      " [ 5.5204]\n",
      " [ 5.0594]\n",
      " [ 5.7077]\n",
      " [ 7.6366]\n",
      " [ 5.8707]\n",
      " [ 5.3054]\n",
      " [ 8.2934]\n",
      " [13.394 ]\n",
      " [ 5.4369]]\n",
      "[[ 6.1101]\n",
      " [ 5.5277]\n",
      " [ 8.5186]\n",
      " [ 7.0032]\n",
      " [ 5.8598]\n",
      " [ 8.3829]\n",
      " [ 7.4764]\n",
      " [ 8.5781]\n",
      " [ 6.4862]\n",
      " [ 5.0546]\n",
      " [ 5.7107]\n",
      " [14.164 ]\n",
      " [ 5.734 ]\n",
      " [ 8.4084]\n",
      " [ 5.6407]\n",
      " [ 5.3794]\n",
      " [ 6.3654]\n",
      " [ 5.1301]\n",
      " [ 6.4296]\n",
      " [ 7.0708]\n",
      " [ 6.1891]\n",
      " [20.27  ]\n",
      " [ 5.4901]\n",
      " [ 6.3261]\n",
      " [ 5.5649]\n",
      " [18.945 ]\n",
      " [12.828 ]\n",
      " [10.957 ]\n",
      " [13.176 ]\n",
      " [22.203 ]\n",
      " [ 5.2524]\n",
      " [ 6.5894]\n",
      " [ 9.2482]\n",
      " [ 5.8918]\n",
      " [ 8.2111]\n",
      " [ 7.9334]\n",
      " [ 8.0959]\n",
      " [ 5.6063]\n",
      " [12.836 ]\n",
      " [ 6.3534]\n",
      " [ 5.4069]\n",
      " [ 6.8825]\n",
      " [11.708 ]\n",
      " [ 5.7737]\n",
      " [ 7.8247]\n",
      " [ 7.0931]\n",
      " [ 5.0702]\n",
      " [ 5.8014]\n",
      " [11.7   ]\n",
      " [ 5.5416]\n",
      " [ 7.5402]\n",
      " [ 5.3077]\n",
      " [ 7.4239]\n",
      " [ 7.6031]\n",
      " [ 6.3328]\n",
      " [ 6.3589]\n",
      " [ 6.2742]\n",
      " [ 5.6397]\n",
      " [ 9.3102]\n",
      " [ 9.4536]\n",
      " [ 8.8254]\n",
      " [ 5.1793]\n",
      " [21.279 ]\n",
      " [14.908 ]\n",
      " [18.959 ]\n",
      " [ 7.2182]\n",
      " [ 8.2951]\n",
      " [10.236 ]\n",
      " [ 5.4994]\n",
      " [20.341 ]\n",
      " [10.136 ]\n",
      " [ 7.3345]\n",
      " [ 6.0062]\n",
      " [ 7.2259]\n",
      " [ 5.0269]\n",
      " [ 6.5479]\n",
      " [ 7.5386]\n",
      " [ 5.0365]\n",
      " [10.274 ]\n",
      " [ 5.1077]\n",
      " [ 5.7292]\n",
      " [ 5.1884]\n",
      " [ 6.3557]\n",
      " [ 9.7687]\n",
      " [ 6.5159]\n",
      " [ 8.5172]\n",
      " [ 9.1802]\n",
      " [ 6.002 ]\n",
      " [ 5.5204]\n",
      " [ 5.0594]\n",
      " [ 5.7077]\n",
      " [ 7.6366]\n",
      " [ 5.8707]\n",
      " [ 5.3054]\n",
      " [ 8.2934]\n",
      " [13.394 ]\n",
      " [ 5.4369]]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Insert a column of ones to the _left_ side of the matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f88c289c1d3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mtheta_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-f88c289c1d3a>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(X, y, theta)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mX_prime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mX_prime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f88c289c1d3a>\u001b[0m in \u001b[0;36madd_column\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Insert a column of ones to the _left_ side of the matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Insert a column of ones to the _left_ side of the matrix"
     ]
    }
   ],
   "source": [
    "# assertions below are true only for this \n",
    "# specific case and are given to ease debugging!\n",
    "\n",
    "def add_column(X):\n",
    "    assert len(X.shape) == 2 and X.shape[1] == 1\n",
    "    print(X)\n",
    "    np.insert(X, 0, 1, axis = 1)\n",
    "    print(X)\n",
    "    raise NotImplementedError(\"Insert a column of ones to the _left_ side of the matrix\")\n",
    "\n",
    "def predict(X, theta):\n",
    "    \"\"\" Computes h(x; theta) \"\"\"\n",
    "    assert len(X.shape) == 2 and X.shape[1] == 1\n",
    "    assert theta.shape == (2, 1)\n",
    "    \n",
    "    X_prime = add_column(X)\n",
    "    pred = None\n",
    "    raise NotImplementedError(\"Compute the regression predictions\")\n",
    "    \n",
    "    return pred\n",
    "\n",
    "def loss(X, y, theta):\n",
    "    assert X.shape == (n, 1)\n",
    "    assert y.shape == (n, 1)\n",
    "    assert theta.shape == (2, 1)\n",
    "    \n",
    "    X_prime = add_column(X)\n",
    "    assert X_prime.shape == (n, 2)\n",
    "    \n",
    "    raise NotImplementedError(\"Compute the model loss; use the predict() function\")\n",
    "    loss = None\n",
    "    return loss\n",
    "\n",
    "theta_init = np.zeros((2, 1))\n",
    "print(loss(X, y, theta_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.3** **[40pt]** GD Implementation\n",
    "\n",
    "Next, you will implement gradient descent. The loop structure has been written for you, and you only need to supply the updates to $\\theta$ within each iteration. \n",
    "\n",
    "As you program, make sure you understand what you are trying to optimize and what is being updated. Keep in mind that the cost is parameterized by the vector $\\theta$ not X and y. That is, we minimize the value of $J(\\theta)$ by changing the values of the vector $\\theta$, not by changing X or y.\n",
    "\n",
    "A good way to verify that gradient descent is working correctly is to look at the value of  and check that it is decreasing with each step. Your value of $J(\\theta)$ should never increase, and should converge to a steady value by the end of the algorithm.  Another way of making sure your gradient estimate is correct is to check it againts a [finite difference](https://en.wikipedia.org/wiki/Finite_difference) approximation.\n",
    "\n",
    "We also initialize the initial parameters to 0 and the learning rate alpha to `0.01`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "from functools import partial\n",
    "\n",
    "def loss_gradient(X, y, theta):\n",
    "    X_prime = add_column(X)\n",
    "    raise NotImplementedError(\"Compute the model loss gradient; \"\n",
    "                              \"use the predict() function; \"\n",
    "                              \"this also must be vectorized!\")\n",
    "    return loss_grad\n",
    "    \n",
    "assert loss_gradient(X, y, theta_init).shape == (2, 1)\n",
    "\n",
    "def finite_diff_grad_check(f, grad, points, eps=1e-10):\n",
    "    errs = []\n",
    "    for point in points:\n",
    "        point_errs = []\n",
    "        grad_func_val = grad(point)\n",
    "        for dim_i in range(point.shape[0]):\n",
    "            diff_v = np.zeros_like(point)\n",
    "            diff_v[dim_i] = eps\n",
    "            dim_grad = (f(point+diff_v) - f(point-diff_v))/(2*eps)\n",
    "            point_errs.append(abs(dim_grad - grad_func_val[dim_i]))\n",
    "        errs.append(point_errs)\n",
    "    return errs\n",
    "\n",
    "test_points = [np.random.rand(2, 1) for _ in range(10)]\n",
    "finite_diff_errs = finite_diff_grad_check(\n",
    "    partial(loss, X, y), partial(loss_gradient, X, y), test_points\n",
    ")\n",
    "\n",
    "print('max grad comp error', np.max(finite_diff_errs))\n",
    "assert np.max(finite_diff_errs) < 1e-3, \"grad computation error is too large\"\n",
    "\n",
    "def run_gd(loss, loss_gradient, X, y, theta_init, lr=0.01, n_iter=1500):\n",
    "    theta_current = theta_init.copy()\n",
    "    loss_values = []\n",
    "    theta_values = []\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        loss_value = loss(X, y, theta_current)\n",
    "        raise NotImplementedError(\"Put update step code here\")\n",
    "        theta_current = None\n",
    "        loss_values.append(loss_value)\n",
    "        theta_values.append(theta_current)\n",
    "        \n",
    "    return theta_current, loss_values, theta_values\n",
    "\n",
    "result = run_gd(loss, loss_gradient, X, y, theta_init)\n",
    "theta_est, loss_values, theta_values = result\n",
    "\n",
    "print('estimated theta value', theta_est.ravel())\n",
    "print('resulting loss', loss(X, y, theta_est))\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iter_i')\n",
    "plt.plot(loss_values)\n",
    "plt.show()\n",
    "\n",
    "plt.ylabel('log(loss)')\n",
    "plt.xlabel('iter_i')\n",
    "plt.semilogy(loss_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.4** **[10pt]** After you are finished, use your final parameters to plot the linear fit. The result should look something like on the figure below. Use the `predict()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X, y, marker='x', color='r', alpha=0.5)\n",
    "x_start, x_end = 5, 25\n",
    "\n",
    "raise NotImplementedError(\"Put code that plots a regression line here\")\n",
    "\n",
    "plt.xlabel('Population in 10\\'000 s')\n",
    "plt.ylabel('Profit in 10\\'000$ s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your final values for $\\theta$ and the `predict()` function to make predictions on profits in areas of 35,000 and 70,000 people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"Predict values given inputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the cost function better, you will now plot the cost over a 2-dimensional grid of values. You will not need to code anything new for this part, but you should understand how the code you have written already is creating these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.cm as cm\n",
    "limits = [(-10, 10), (-1, 4)]\n",
    "space = [np.linspace(*limit, 100) for limit in limits]\n",
    "theta_1_grid, theta_2_grid = np.meshgrid(*space)\n",
    "theta_meshgrid = np.vstack([theta_1_grid.ravel(), theta_2_grid.ravel()])\n",
    "loss_test_vals_flat = (((add_column(X) @ theta_meshgrid - y)**2).mean(axis=0)/2)\n",
    "loss_test_vals_grid = loss_test_vals_flat.reshape(theta_1_grid.shape)\n",
    "print(theta_1_grid.shape, theta_2_grid.shape, loss_test_vals_grid.shape)\n",
    "\n",
    "plt.gca(projection='3d').plot_surface(theta_1_grid, theta_2_grid, \n",
    "                                      loss_test_vals_grid, cmap=cm.viridis,\n",
    "                                      linewidth=0, antialiased=False)\n",
    "xs, ys = np.hstack(theta_values).tolist()\n",
    "zs = np.array(loss_values)\n",
    "plt.gca(projection='3d').plot(xs, ys, zs, c='r')\n",
    "plt.xlim(*limits[0])\n",
    "plt.ylim(*limits[1])\n",
    "plt.show()\n",
    "\n",
    "plt.contour(theta_1_grid, theta_2_grid, loss_test_vals_grid, levels=np.logspace(-2, 3, 20))\n",
    "plt.plot(xs, ys)\n",
    "plt.scatter(xs, ys, alpha=0.005)\n",
    "plt.xlim(*limits[0])\n",
    "plt.ylim(*limits[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Linear regression with multiple input features\n",
    "\n",
    "**3.1** **[20pt]** Copy-paste your `add_column`, `predict`, `loss` and `loss grad` implementations from above and modify your code of linear regression with one variable to support any number of input features (vectorize your code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('ex1data2.txt', delimiter=',')\n",
    "X, y = data[:, :-1], data[:, -1, np.newaxis]\n",
    "n = data.shape[0]\n",
    "print(X.shape, y.shape, n)\n",
    "print(X[:10], '\\n', y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"Implement new add_column(), predict(), loss(), loss_gradient() here for multivariate regression\")\n",
    "\n",
    "\n",
    "theta_init = np.zeros((3, 1))\n",
    "result = run_gd(loss, loss_gradient, X, y, theta_init, n_iter=10000, lr=1e-10)\n",
    "theta_est, loss_values, theta_values = result\n",
    "plt.plot(loss_values)\n",
    "plt.show()\n",
    "\n",
    "# raise NotImplementedError(\"Put your multivariate regression code here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2** **[20pt]** Draw a histogam of values for the first and second feature. Why is feature normalization important? Normalize features and re-run the gradient decent. Compare loss plots that you get with and without feature normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"Draw histogram for values of feature 1\")\n",
    "plt.show()\n",
    "\n",
    "raise NotImplementedError(\"Draw histogram for values of feature 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "theta_init = np.zeros((3, 1))\n",
    "X_normed = np.zeros_like(X)\n",
    "raise NotImplementedError(\"Run gd on normalized versions of feature vectors\")\n",
    "result = run_gd(loss, loss_gradient, X_normed, y, theta_init, n_iter=10000, lr=1e-3)\n",
    "theta_est, loss_values, theta_values = result\n",
    "\n",
    "plt.plot(loss_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 [10pt]** How can we choose an appropriate learning rate? See what will happen if the learning rate is too small or too large for normalized and not normalized cases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"Plot loss behaviour when with multiple different learning rates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Written part\n",
    "\n",
    "These problems are extremely important preparation for the exam. Submit solutions to each problem by filling the markdown cells below.\n",
    "\n",
    "**4.1 [10 pt]** Maximum Likelihood Estimate for Coin Toss\n",
    "\n",
    "The probability distribution of a single binary variable  that takes value  with probability  is given by the Bernoulli distribution\n",
    "\n",
    "$$ \\operatorname{Bern}(x|\\mu) = \\mu^x (1-\\mu)^{1-x}$$\n",
    "\n",
    "For example, we can use it to model the probability of seeing ‘heads’ ($x=1$) or ‘tails’ ($x=0$) after tossing a coin, with $\\mu$ being the probability of seeing ‘heads’. Suppose we have a dataset of independent coin flips $D = \\{ x^{(1)} , \\dots, x^{(m)} \\}$ and we would like to estimate $\\mu$ using Maximum Likelihood. Recall that we can write down the likelihood function as\n",
    "\n",
    "$$\\mathcal L (x^{(i)}|\\mu) = \\mu^{x^{(i)}} (1-\\mu)^{1-{x^{(i)}}}$$\n",
    "\n",
    "$$ P(D|\\mu) = \\prod_i \\mathcal L (x^{(i)}|\\mu)$$\n",
    "\n",
    "The log of the likelihood function is\n",
    "\n",
    "$$ \\ln P(D|\\mu) = \\sum_i x^{(i)} \\ln \\mu + (1-x^{(i)})\\ln(1-\\mu)$$\n",
    "\n",
    "Show that the ML solution for $\\mu$ is given by  $\\mu_{ML} = \\frac{h}{m}$  where $h$ is the total number of ‘heads’ in the dataset. Show all of your steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution** \n",
    "\n",
    "Given that h is the total number of 'heads' in the dataset, (m-h) is the total number of 'tails' in the dataset.\n",
    "$$\n",
    "\\ln P(D|\\mu) = \\sum_i x^{(i)} \\ln \\mu + (1-x^{(i)})\\ln(1-\\mu) = h \\ln \\mu + (m - h)\\ln(1-\\mu)\n",
    "$$\n",
    "Differentiate with $\\mu$:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mu}\\ln P(D|\\mu) = \\frac{h}{\\mu} - \\frac{m-h}{1-\\mu} \\\\ = \\frac{h - \\mu m}{\\mu(1 - \\mu)}\n",
    "$$\n",
    "Since $\\mu \\in (0,1)$, when $\\mu \\in (0, \\frac{h}{m})$, we have $\\frac{\\partial}{\\partial \\mu}\\ln P(D|\\mu) > 0$. When $\\mu \\in (\\frac{h}{m},1)$, we have $\\frac{\\partial}{\\partial \\mu}\\ln P(D|\\mu) < 0$. \\\\\n",
    "Therefore, when $\\mu_{ML} = \\frac{h}{m}$, the likelihood function is maximized, which means the ML solution for $\\mu$ is given by $\\mu_{ML} = \\frac{h}{m}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 [10 pt]** Localized linear regression\n",
    "\n",
    "Suppose we want to estimate localized linear regression by weighting the contribution of the data points by their distance to the query point $x_q$, i.e. using the cost\n",
    "\n",
    "$$ E(x_q) = \\frac{1}{2}\\sum_i^m \\frac{(y^{(i)}-h(x^{(i)}|\\theta))^2}{|| x^{(i)} - x_q ||^2}$$\n",
    "\n",
    "where $\\frac{1}{|| x^{(i)} - x_q ||^2} = w^{(i)}$ is the inverse Euclidean distance between the training point $x^{(i)}$ and query (test) point $x_q$.\n",
    "\n",
    "Derive the modified normal equations for the above cost function $E(x_q)$. Hint: first, re-write the cost function in matrix/vector notation, using a diagonal matrix to represent the weights $w^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your solution**\n",
    "\n",
    "Given that $w^{(i)} = \\frac{1}{|| x^{(i)} - x_q ||^2}$, we have $E(x_q) = \\frac{1}{2}  \\sum_{i=1}^m w^{(i)}(y^{(i)}-h(x^{(i)}|\\theta))^2$.\n",
    "\t$$\n",
    "\tW=\n",
    "\t\\begin{bmatrix}\n",
    "\tw^{1} & 0  & \\cdots &  0   \\\\\n",
    "\t0 & w^{2}  & \\cdots &  0  \\\\\n",
    "\t\\vdots & \\vdots  & \\ddots   & \\vdots  \\\\\n",
    "\t0 & 0  & \\cdots\\  & w^{m}  \\\\\n",
    "\t\\end{bmatrix}\n",
    "\t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3 [10 pt]** Betting on Trick Coins\n",
    "\n",
    "A game is played with three coins in a jar: one is a normal coin, one has “heads” on both sides, one has “tails” on both sides. All coins are “fair”, i.e. have equal probability of landing on either side. Suppose one coin is picked randomly from the jar and tossed, and lands with “heads” on top. What is the probability that the bottom side is also “heads”? Show all your steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**solution**\n",
    "\n",
    "Define event X as \"The coin with 'heads' on both sides is picked\".\n",
    "\n",
    "Define event Y as \"Pick randomly from the jar and toss, land with 'heads' on top\".\n",
    "\n",
    "According to the Bayesian rule:\n",
    "$$\n",
    "P(X | Y) = \\frac{P(Y | X)P(X)}{P(Y)}\n",
    "$$\n",
    "It can be derived that: $ P(X | Y) = 1 $, $ P(X) = \\frac{1}{3} $, $P(Y) = \\frac{1}{3}*\\frac{1}{2} + \\frac{1}{3}*1 = \\frac{1}{2}$. \n",
    "\n",
    "Therefore, we have:\n",
    "$$\n",
    "P(X | Y) = \\frac{P(Y | X)P(X)}{P(Y)} = \\frac{1 * \\frac{1}{3}}{\\frac{1}{2}} = \\frac{2}{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
