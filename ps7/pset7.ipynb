{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "Ekjsmds4_n2G",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem Set 7 (Total points: 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VxmzV7JUbHjx"
   },
   "source": [
    "### Q1. Markov Decision Process [20 points]\n",
    "Imagine an MDP corresponding to playing slot machines in a casino. Suppose you start with \\\\$20 cash to spend in the casino,  and you decide to play until you lose all your money or until you double your money (i.e., increase your cash to at least \\\\$40). There are two slot machines you can choose to play: 1) slot machine X costs \\\\$10 to play and will pay out \\\\$20 with probability 0.05 and will pay \\\\$0 otherwise;\n",
    "and 2) slot machine Y costs \\\\$20 to play and will pay out \\\\$30 with probability 0.01 and \\\\$0 otherwise. As you are playing, you keep choosing machine X or Y at each turn.\n",
    "\n",
    "Write down the MDP that corresponds to the above problem. Clearly specify the state space, action space, rewards and transition probabilities. Indicate which state(s) are terminal. Assume that the discount factor γ = 1.\n",
    "\n",
    "**Notes:** There are several valid ways to specify the MDP, so you have some flexibility in your solution. For example, rewards can take many different forms, but overall you should get a higher reward for stopping when you double your money than when you lose all your money!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Solution***:\n",
    "\n",
    "State Space S:\n",
    "\n",
    "{no cash left, $\\$$10 cash left, $\\$$20 cash left, $\\$$30 cash left, $\\$$40 cash left}.\n",
    "\n",
    "\n",
    "The MDP will be over when the state is \"no cash left\" or \"$\\$$40 cash left\".\n",
    "\n",
    "\n",
    "Action Space A: \n",
    "\n",
    "{Choose to play slot machine X, Choose to play slot machine Y}.\n",
    "\n",
    "Reward R:\n",
    "\n",
    "R($\\$$10 cash left, choose to play X, no cash left) = -50\n",
    "\n",
    "R($\\$$10 cash left, choose to play X, $\\$$20 cash left) = 10  \n",
    "\n",
    "R($\\$$20 cash left, choose to play X, $\\$$10 cash left) = -10 \n",
    "\n",
    "R($\\$$20 cash left, choose to play X, $\\$$30 cash left) = 10\n",
    "\n",
    "R($\\$$20 cash left, choose to play Y, no cash left) = -50\n",
    "\n",
    "R($\\$$20 cash left, choose to play Y, $\\$$30 cash left) = 10\n",
    "\n",
    "R($\\$$30 cash left, choose to play X, $\\$$20 cash left) = -10\n",
    "\n",
    "R($\\$$30 cash left, choose to play X, $\\$$40 cash left) = 100\n",
    "\n",
    "R($\\$$30 cash left, choose to play Y, $\\$$10 cash left) = -10\n",
    "\n",
    "R($\\$$30 cash left, choose to play Y, $\\$$40 cash left) = 100\n",
    "\n",
    "Transition Probability $P_{sa}$:\n",
    "\n",
    "P($\\$$10 cash left, choose to play X, no cash left) = 0.95 \n",
    "\n",
    "P($\\$$10 cash left, choose to play X, $\\$$20 cash left) = 0.05 \n",
    "\n",
    "P($\\$$20 cash left, choose to play X, $\\$$10 cash left) = 0.95 \n",
    "\n",
    "P($\\$$20 cash left, choose to play X, $\\$$30 cash left) = 0.05\n",
    "\n",
    "P($\\$$20 cash left, choose to play Y, no cash left) = 0.99\n",
    "\n",
    "P($\\$$20 cash left, choose to play Y, $\\$$30 cash left) = 0.01\n",
    "\n",
    "P($\\$$30 cash left, choose to play X, $\\$$20 cash left) = 0.95 \n",
    "\n",
    "P($\\$$30 cash left, choose to play X, $\\$$40 cash left) = 0.05\n",
    "\n",
    "P($\\$$30 cash left, choose to play Y, $\\$$10 cash left) = 0.99\n",
    "\n",
    "P($\\$$30 cash left, choose to play Y, $\\$$40 cash left) = 0.01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_H3z6wR_n2I"
   },
   "source": [
    "## Reinforcement Learning\n",
    "In the remainder of the problem set you will implement the Q-Learning Algorithm to solve the \"Frozen Lake\" problem.\n",
    "We​ ​will​ ​use​ ​OpenAI’s​ ​gym​ ​package​ ​to​ ​develop​ ​our​ ​solution​ ​in​ ​Python.​ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q8sgYT6oUKGz"
   },
   "source": [
    "### OpenAI Gym Setup \n",
    "​Read the​ ​set-up​ ​instructions for​ ​Gym​  [here](https://gym.openai.com/docs/).​ ​The​ ​instructions​ ​also​ ​give​ ​a​ ​good​ ​overview​ ​of​ ​the​ ​API​ ​for​ ​this​ ​package,​ ​so​ ​please​ ​read through​ ​it​ ​before​ ​proceeding.​\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pQ-qqj1n_n2K",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Frozen Lake\n",
    "Instead​ ​of​ ​using​ ​CartPole,​ ​we’re​ ​going​ ​to​ ​be​ ​using​ ​the​ [​​FrozenLake](https://gym.openai.com/envs/FrozenLake-v0/) environment. Read through the code for this environment by following the Github link.​ <br>\n",
    "\n",
    "Winter​ ​is​ ​quickly​ ​approaching,​ ​and​ ​we​ ​have​ ​to​ ​worry​ ​about​ ​navigating​ ​frozen​ ​lakes.​ ​It’s​ ​only early November,​ ​\n",
    "so​ ​the​ ​lakes​ ​haven’t​ ​completely​ ​frozen​ ​and​ ​if​ ​you​ ​make​ ​the​ ​wrong​ ​step​ ​you​ ​may​ ​fall​ ​through. \n",
    "We’ll​ ​need​ ​to​ ​learn​ ​how​ ​to​ ​get​ ​to​ ​our​ ​destination​ ​when​ ​stuck​ ​on​ ​the​ ​ice,​ ​without​ ​falling​ ​in.\n",
    "The​ ​lake​ ​we’re​ ​going​ ​to​ ​consider​ ​is a​ ​square​ ​lake​ ​with​ ​spots​ ​to​ ​step​ ​on​ ​in​ ​the​ ​shape​ ​of​ ​a​ ​grid.​ ​​<br>\n",
    "\n",
    "The surface is described using a 4x4 grid like the following\n",
    "\n",
    "        S F F F \n",
    "        F H F H\n",
    "        F F F H\n",
    "        H F F G\n",
    "\n",
    "​Each​ ​spot​ ​can have​ ​one​ ​of​ ​four​ ​states:\n",
    "- S:​ ​starting​ ​point.\n",
    "- G:​ ​goal​ ​point.\n",
    "- F:​ ​frozen​ ​spot,​ ​where​ ​it’s​ ​safe​ ​to​ ​walk.\n",
    "- H:​ ​hole​ ​in​ ​the​ ​ice,​ ​where​ ​it’s​ ​not​ ​safe​ ​to​ ​walk.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4fPxEMPQ_n2L"
   },
   "source": [
    "For example, consider the lake, <br>\n",
    "![alt text](https://cs-people.bu.edu/sunxm/frozen_lake_example.png)\n",
    "\n",
    "There are four possible actions: UP, DOWN, LEFT, RIGHT. Although we​ ​can​ ​see​ ​the​ ​path​ ​we​ ​need​ ​to​ ​walk,​ the agent does not. ​We’re​ ​going​ ​to​ ​train​ ​an​ ​agent​ ​to​ ​discover​ ​this​ ​via​ ​problem solving.​ ​However,​ ​walking​ ​on​ ​ice​ ​isn’t​ ​so​ ​easy!​ ​Sometimes​ ​you​ ​slip​ ​and​ ​aren’t​ ​able​ ​to​ ​take​ ​the​ ​step​ ​you intended.\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole.\n",
    "\n",
    "You receive a reward of 1 if you reach the goal, and zero otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ZlKYHn-XE2X"
   },
   "source": [
    "### Q2. Walking on the Frozen Lake [10 points]\n",
    "\n",
    "Write a script that sets up the Frozen Lake environment and takes 10 walks through it, consisting of a maximum 10 randomly sampled actions during each walk. After each step, render the current state, and print out the reward and whether the walk is \"done\", i.e. in the terminal state, because the agent fell into a hole (stop if it is). In your own words, explain how this environment behaves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /Users/simple/opt/anaconda3/lib/python3.8/site-packages (0.21.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/simple/opt/anaconda3/lib/python3.8/site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/simple/opt/anaconda3/lib/python3.8/site-packages (from gym) (1.20.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: True\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: True\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Down)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: True\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: True\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: True\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Down)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Left)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: True\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Left)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "Done: True\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('FrozenLake-v1').unwrapped\n",
    "env.reset()\n",
    "for i_episode in range(10):\n",
    "    observation = env.reset()\n",
    "    for j in range(10):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action) \n",
    "        print(\"Reward:\", reward)\n",
    "        print(\"Done:\", done)\n",
    "        if done:\n",
    "            break;\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Down)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: True\n",
    "\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Right)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Up)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Left)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Left)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Up)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: True\n",
    "\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Up)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Down)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Right)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: True\n",
    "\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Down)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Right)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: True\n",
    "\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Down)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Left)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Down)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Left)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Left)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Up)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Left)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Right)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Left)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Right)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: True\n",
    "\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Left)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Right)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Up)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Up)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Up)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Left)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Up)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Down)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Up)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Left)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Up)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Right)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Down)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Up)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Left)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Up)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Left)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Left)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Left)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: True\n",
    "\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Left)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Down)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Right)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Left)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: False\n",
    "  (Left)\n",
    "SFFF\n",
    "FHFH\n",
    "FFFH\n",
    "HFFG\n",
    "Reward: 0.0\n",
    "Done: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CHPqSDaE_n2M"
   },
   "source": [
    "### Q3. Q-Learning [50 points]\n",
    "\n",
    "You will ​implement​ ​Q-learning to solve the problem.​ Assume that the environment has states S and actions A. Use the ​function​ ​signature​ ​provided​ below:\n",
    "\n",
    "``` python\n",
    "def​ q_learning(env,​ ​alpha=0.5,​ gamma=0.95,​ ​epsilon=0.1, num_episodes=500):\n",
    "\"\"\" ​Performs​ ​Q-learning​ ​for​ ​the​ ​given​ ​environment.\n",
    "Initialize​ ​Q​ ​to​ ​all​ ​zeros.\n",
    ":param​ ​env:​ ​Unwrapped​ ​OpenAI​ ​gym​ ​environment.\n",
    ":param​ ​alpha:​ ​Learning​ ​rate​ ​parameter.\n",
    ":param​ ​gamma:​ ​Decay​ ​rate​ (future reward discount) ​parameter.\n",
    ":param​ ​num_episodes:​ ​Number​ ​of​ ​episodes​ ​to​ ​use​ ​for​ ​learning. \n",
    ":return:​ ​Q​ ​table, i.e. a table with the Q value for every <S, A> pair.\n",
    "\"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgFa7qID_n2O",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The pseudocode for Q-Learning was described in lecture, but for your reference, we provide it here:\n",
    "![alt text](https://cs-people.bu.edu/sunxm/q-learning.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, alpha=0.5, gamma=0.95, epsilon=0.1, num_episodes=500, random_init = False):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    if random_init == False:\n",
    "        q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    else:    \n",
    "        q_table = np.random.rand(env.observation_space.n, env.action_space.n)\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        \n",
    "        while not done:\n",
    "            prob = np.random.uniform(0,1)\n",
    "            if prob > epsilon:\n",
    "                if q_table[state,0] == q_table[state,1] == q_table[state,2] == q_table[state,3]:\n",
    "                    action = int(np.random.randint(4,size = 1))\n",
    "                else:\n",
    "                    action = np.argmax(q_table[state,:])\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "                \n",
    "            new_state, reward, done, info = env.step(action) \n",
    "            max_Q = np.max(q_table[new_state,:])\n",
    "            q_table[state][action] = q_table[state][action] + alpha * ((reward + (gamma * max_Q) - q_table[state][action]))\n",
    "            state = new_state\n",
    "            \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1JHa8uv6_n2N"
   },
   "source": [
    "### Q4. Main Function [20 points]\n",
    "You also need to implement the main function to solve the entire FrozenLake-v0 problem, including setting up the Gym environment, \n",
    "calling the q-learning function as defined above, printing out the returned Q-table, etc. <br>\n",
    "\n",
    "You should use the $\\epsilon$-greedy algorithm (solving Exploration-Exploitation Dilemma) to generate actions for each state. Try `num_episodes` with different values, e.g. `num_episodes=500, 1000, 5000, 10000, 50000`. You should also try different initializations for the Q-table, i.e. Random Initialization and Zero Initialization. \n",
    "\n",
    "Please do not change any default value of environment setting, such as `is_slippery`, if not mentioned above.\n",
    "\n",
    "Provide​ ​the​ ​final​ ​Q-table​ ​of​ ​​FrozenLake-v0​ for **each <num_episode, init_method>** you have tried [10 points],​ ​and analyze ​what​ you observe [10 points]. \n",
    "\n",
    "If you are interested in testing your learned Q-learned, you can compute the win rate (the probability of agents to goal following the Q-table). Note that you should use `argmax` instead of epsilon greedy to choose the action at the current state. \n",
    "Win Rate is a good way to test whether you write your algorithm correctly.\n",
    "\n",
    "Just for your reference, we test our implementation for 10000 trials after 50000-episode training using zero initialization and the win rate is 0.4985. You should achieve similar value under the same test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize the Q-table with zeros\n",
      "The number of episodes: 500\n",
      "[[0.15824509 0.10136999 0.12086642 0.11701772]\n",
      " [0.06022394 0.03911476 0.01309797 0.09769829]\n",
      " [0.04535202 0.0520334  0.04628593 0.05773853]\n",
      " [0.0344887  0.04124574 0.03526499 0.05063952]\n",
      " [0.20384944 0.07070142 0.035757   0.03660859]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.0712186  0.01421652 0.01776473 0.00690219]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.19228369 0.10167316 0.16181201 0.29650994]\n",
      " [0.12668874 0.39300645 0.06140277 0.17213502]\n",
      " [0.22334252 0.0780073  0.         0.00981819]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.19242893 0.16306879 0.68723293 0.50597705]\n",
      " [0.62470577 0.61907223 0.52201677 0.8495072 ]\n",
      " [0.         0.         0.         0.        ]]\n",
      "The number of episodes: 1000\n",
      "[[0.15257714 0.11906468 0.14514301 0.13035266]\n",
      " [0.0573819  0.05553607 0.02828044 0.08818235]\n",
      " [0.06581191 0.06055998 0.06331068 0.07797309]\n",
      " [0.04856126 0.04473507 0.04581002 0.07806577]\n",
      " [0.15930546 0.04283418 0.16322927 0.08963268]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.0172129  0.01138595 0.01920685 0.00331315]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.10104626 0.02719556 0.11818231 0.27338463]\n",
      " [0.27688206 0.22551539 0.57752529 0.25458551]\n",
      " [0.13136779 0.77908327 0.02844816 0.08798455]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.28334374 0.30666815 0.71372747 0.43683866]\n",
      " [0.54676979 0.98615947 0.65825753 0.77143423]\n",
      " [0.         0.         0.         0.        ]]\n",
      "The number of episodes: 5000\n",
      "[[1.69261944e-01 1.52046177e-01 1.02285938e-01 1.05592544e-01]\n",
      " [5.19660603e-02 2.67683300e-02 5.62663600e-02 1.01196987e-01]\n",
      " [7.36459534e-02 9.32462433e-02 1.33751517e-01 9.23723744e-02]\n",
      " [4.78854543e-02 5.46340435e-02 8.41382135e-02 9.43376747e-02]\n",
      " [2.42410485e-01 8.53320603e-02 4.28053200e-02 1.38861609e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.06847839e-02 5.72356793e-05 2.06898296e-01 1.25250683e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [6.35914213e-02 1.51362099e-01 5.52812831e-02 3.87055503e-01]\n",
      " [1.52152348e-01 4.07132148e-01 1.94075298e-01 1.66028481e-01]\n",
      " [5.42978942e-01 1.21961815e-01 1.37353392e-01 1.91529112e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.57310061e-01 2.17860030e-01 4.86267509e-01 1.68550849e-01]\n",
      " [3.79188722e-01 4.29747840e-01 7.13174171e-01 6.04680549e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
      "The number of episodes: 10000\n",
      "[[0.20593457 0.28217816 0.15465964 0.16159984]\n",
      " [0.02738237 0.08248517 0.13197156 0.19447891]\n",
      " [0.07583954 0.09765528 0.09692445 0.11323119]\n",
      " [0.07685807 0.02222686 0.06791594 0.10237254]\n",
      " [0.34519721 0.16223084 0.13424478 0.11262247]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.01881663 0.01724667 0.02571974 0.03041565]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.21962662 0.27428177 0.10429337 0.38458971]\n",
      " [0.24857144 0.40820861 0.14750567 0.00871324]\n",
      " [0.229893   0.16922814 0.06060432 0.01476437]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.12545141 0.40602148 0.54890509 0.04931889]\n",
      " [0.50928746 0.64064345 0.61815749 0.51644746]\n",
      " [0.         0.         0.         0.        ]]\n",
      "The number of episodes: 50000\n",
      "[[0.1608676  0.1645345  0.26027068 0.17633794]\n",
      " [0.06812437 0.03071576 0.0263692  0.13955854]\n",
      " [0.10349604 0.12560558 0.08560064 0.05906963]\n",
      " [0.00534941 0.01567526 0.04892368 0.11467984]\n",
      " [0.35397231 0.2451174  0.06803224 0.07214201]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.07353203 0.0952675  0.17567439 0.06615013]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05628672 0.08265295 0.23477713 0.3677654 ]\n",
      " [0.45652281 0.3593186  0.08889338 0.35178368]\n",
      " [0.18859333 0.20583196 0.51972437 0.03032964]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.44579143 0.26397149 0.69321331 0.45540548]\n",
      " [0.60054941 0.94422073 0.70564583 0.49295497]\n",
      " [0.         0.         0.         0.        ]]\n",
      "Initialize the Q-table with random number between 0 and 1\n",
      "The number of episodes: 500\n",
      "[[0.5125593  0.49293603 0.48394278 0.50559246]\n",
      " [0.49204806 0.5062872  0.50833106 0.49919264]\n",
      " [0.48289115 0.5014876  0.47524902 0.49718576]\n",
      " [0.46645305 0.21233911 0.52237308 0.46678048]\n",
      " [0.56330846 0.56352373 0.69951064 0.54168802]\n",
      " [0.61185289 0.13949386 0.29214465 0.36636184]\n",
      " [0.55492142 0.58045541 0.44079645 0.51414811]\n",
      " [0.59241457 0.04645041 0.60754485 0.17052412]\n",
      " [0.65651937 0.88737167 0.77474198 0.66714165]\n",
      " [0.51209863 0.56700243 1.16985741 0.5434952 ]\n",
      " [0.33504771 0.49517691 0.33189235 0.62229234]\n",
      " [0.25877998 0.66252228 0.31171108 0.52006802]\n",
      " [0.54671028 0.18485446 0.96958463 0.77513282]\n",
      " [0.70414977 1.53023237 0.59628538 1.0361151 ]\n",
      " [0.0884925  0.19598286 0.04522729 1.75738816]\n",
      " [0.38867729 0.27134903 0.82873751 0.35675333]]\n",
      "The number of episodes: 1000\n",
      "[[0.47716359 0.69779809 0.48082496 0.48316653]\n",
      " [0.48659374 0.50283918 0.55734035 0.5004835 ]\n",
      " [0.46005907 0.48932572 0.46671533 0.4899798 ]\n",
      " [0.55036579 0.48596048 0.41259039 0.45949868]\n",
      " [0.71025997 0.51407924 0.87163281 0.50452611]\n",
      " [0.61185289 0.13949386 0.29214465 0.36636184]\n",
      " [0.60943844 0.60866337 0.53407442 0.50996509]\n",
      " [0.59241457 0.04645041 0.60754485 0.17052412]\n",
      " [0.70473886 0.92285524 0.77648644 0.8548246 ]\n",
      " [0.73879161 1.17601892 0.63269702 0.70306126]\n",
      " [1.03506945 0.49517691 0.7948209  0.54016996]\n",
      " [0.25877998 0.66252228 0.31171108 0.52006802]\n",
      " [0.54671028 0.18485446 0.96958463 0.77513282]\n",
      " [0.795996   1.16578924 0.93860136 1.06643866]\n",
      " [0.77018611 1.1181331  0.04522729 1.55103533]\n",
      " [0.38867729 0.27134903 0.82873751 0.35675333]]\n",
      "The number of episodes: 5000\n",
      "[[0.51022411 0.51121526 0.58625417 0.51076742]\n",
      " [0.49320689 0.4994261  0.59543664 0.49997891]\n",
      " [0.50296181 0.82008831 0.488103   0.48656106]\n",
      " [0.57145541 0.50917191 0.52879918 0.49456924]\n",
      " [0.77236696 0.57203545 0.57871965 0.57667272]\n",
      " [0.61185289 0.13949386 0.29214465 0.36636184]\n",
      " [0.55468728 0.76633947 0.57102833 0.5702907 ]\n",
      " [0.59241457 0.04645041 0.60754485 0.17052412]\n",
      " [0.73043097 0.89334291 0.79035262 0.72996946]\n",
      " [0.67051702 0.90675582 0.61290751 0.80804445]\n",
      " [0.95261544 0.75653326 0.78645214 0.82334929]\n",
      " [0.25877998 0.66252228 0.31171108 0.52006802]\n",
      " [0.54671028 0.18485446 0.96958463 0.77513282]\n",
      " [0.91624941 0.93462018 0.91088457 0.90751009]\n",
      " [1.02532715 1.22197921 1.36973628 1.06452433]\n",
      " [0.38867729 0.27134903 0.82873751 0.35675333]]\n",
      "The number of episodes: 10000\n",
      "[[0.55437825 0.5783342  0.55675629 0.55100733]\n",
      " [0.50355236 0.57481142 0.52200233 0.55221746]\n",
      " [0.52713621 0.65102864 0.48997613 0.50085371]\n",
      " [0.55518334 0.48735576 0.48752242 0.51346034]\n",
      " [0.6101655  0.59692958 0.60734381 0.60365919]\n",
      " [0.61185289 0.13949386 0.29214465 0.36636184]\n",
      " [0.57271293 0.62369719 0.73318344 0.57893545]\n",
      " [0.59241457 0.04645041 0.60754485 0.17052412]\n",
      " [0.84222058 0.93289512 0.83827661 0.67116812]\n",
      " [0.62076639 1.05516684 0.63853128 0.62522839]\n",
      " [1.14368578 0.65387642 0.63082194 0.64234857]\n",
      " [0.25877998 0.66252228 0.31171108 0.52006802]\n",
      " [0.54671028 0.18485446 0.96958463 0.77513282]\n",
      " [0.92689296 0.99061516 0.97274818 0.96329795]\n",
      " [1.04358984 1.59417996 1.24583613 1.25747572]\n",
      " [0.38867729 0.27134903 0.82873751 0.35675333]]\n",
      "The number of episodes: 50000\n",
      "[[0.60984324 0.49367596 0.51723643 0.51474535]\n",
      " [0.52138037 0.54016012 0.5289769  0.50266004]\n",
      " [0.529224   0.632556   0.54031295 0.53254385]\n",
      " [0.47938496 0.55826876 0.50834    0.49800918]\n",
      " [0.65293248 0.56773996 0.52904531 0.56679427]\n",
      " [0.61185289 0.13949386 0.29214465 0.36636184]\n",
      " [0.6515293  0.62090828 0.62270728 0.57250445]\n",
      " [0.59241457 0.04645041 0.60754485 0.17052412]\n",
      " [0.74751256 0.72167061 0.73673793 0.71978531]\n",
      " [0.64786808 0.85344623 0.61784274 0.71587728]\n",
      " [0.70761387 0.82931531 0.71433513 0.93312891]\n",
      " [0.25877998 0.66252228 0.31171108 0.52006802]\n",
      " [0.54671028 0.18485446 0.96958463 0.77513282]\n",
      " [0.83276834 1.09262676 0.83815613 0.87288046]\n",
      " [1.05401669 1.0343398  0.98659764 1.40694441]\n",
      " [0.38867729 0.27134903 0.82873751 0.35675333]]\n"
     ]
    }
   ],
   "source": [
    "num_episode = [500, 1000, 5000, 10000, 50000]\n",
    "\n",
    "env = gym.make('FrozenLake-v1').unwrapped\n",
    "\n",
    "print(\"Initialize the Q-table with zeros\")\n",
    "for i in num_episode:  \n",
    "    print(\"The number of episodes:\", i)\n",
    "    print(q_learning(env, 0.5, 0.95, 0.1, i, False))\n",
    "  \n",
    "print(\"Initialize the Q-table with random number between 0 and 1\")\n",
    "for j in num_episode: \n",
    "    print(\"The number of episodes:\", j)\n",
    "    print(q_learning(env, 0.5, 0.95, 0.1, j, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FtF6U2vg_n2O"
   },
   "source": [
    "#### Additional Instructions\n",
    "If​ ​you’re​ ​new​ ​to​ OpenAI’s​ ​Gym,​ first ​go​ ​through​ ​OpenAI’s​ ​full​ ​tutorial listed​ ​earlier​ ​and​ ​visit​ ​the​ ​Appendix​ ​to​ ​this​ ​homework​ ​before​ ​proceeding.\n",
    "Some additional rules:\n",
    "- Only submit **original code** written entirely by you.\n",
    "- **Permitted​​ non-standard ​​libraries**:​​ ​gym​,​​ ​numpy.\n",
    "- **Only​ ​use​ ​numpy​ ​for​ ​random​ ​sampling,​ ​and​ ​seed​ ​at​ ​the​ ​beginning​ ​of​ ​each​ ​function​ ​with**:\n",
    "np.random.seed(42)\n",
    "- **Unwrap​ ​the​ ​OpenAI​ ​gym​ ​before​ ​providing​ ​them​ ​to​ ​these​ ​functions.** <br>\n",
    "env​ ​=​ ​gym.make(“FrozenLake-v0”).unwrapped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RjpJpTbV_n2Q"
   },
   "source": [
    "## Appendix\n",
    "\n",
    "This​ ​appendix​ ​includes​ ​references​ ​to​ ​APIs​ ​you​ ​may​ ​find​ ​useful.​ ​If​ ​the​ ​description​ ​sounds​ ​useful,​ ​check out\n",
    "the​ ​respective​ ​package’s​ ​documentation​ ​for​ ​a​ ​much​ ​better​ ​description​ ​than​ ​we​ ​could​ ​provide.\n",
    "\n",
    "#### Numpy\n",
    "- np.zeros:​ ​N-dimensional​ ​tensor​ ​initialized​ ​to​ ​all​ ​0s.\n",
    "- np.ones:​ ​N-dimensional​ ​tensor​ ​initialized​ ​to​ ​all​ ​1s.\n",
    "- np.eye:​ ​N-dimensional​ ​tensor​ ​initialized​ ​to​ ​a​ ​diagonal​ ​matrix​ ​of​ ​1s.\n",
    "- np.random.choice:​ ​Randomly​ ​sample​ ​from​ ​a​ ​list,​ ​allowing​ ​you​ ​to​ ​specify​ ​weights.\n",
    "- np.argmax:​ ​Index​ ​of​ ​the​ ​maximum​ ​element.\n",
    "- np.abs:​ ​Absolute​ ​value.\n",
    "- np.mean:​ ​Average​ ​across​ ​dimensions.\n",
    "- np.sum:​ ​Sum​ ​across​ ​dimensions.\n",
    "\n",
    "### OpenAI Gym\n",
    "- Environment​ ​(unwrapped):<br>\n",
    "env.nS #​ ​Number​ ​of​ ​spaces. <br>\n",
    "env.nA #​ ​Number​ ​of​ ​actions. <br>\n",
    "env.P #​ ​Dynamics​ ​model.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pset7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
